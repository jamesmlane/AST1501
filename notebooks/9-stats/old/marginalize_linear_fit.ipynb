{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analytically marginalizating the parameters of a linear fit\n",
    "\n",
    "We are attempting to re-write the log likelihood = minus one-half chi-squared for a linear fit from the form $\\chi^2 = ({\\bf Y} - {\\bf A X})^T {\\bf C}^{-1}\\,({\\bf Y} - {\\bf A X})$ to a form $\\chi^2 = ({\\bf X} - {\\bf W})^T {\\bf V}^{-1}\\,({\\bf X} - {\\bf W}) + {\\bf U}$. We will test this with the data from *Hogg et al. (2010)*. First we load this data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datastring= \"\"\"1 & 201 & 592 & 61\n",
    "               2 & 244 & 401 & 25\n",
    "               3 & 47 & 583 & 38\n",
    "               4 & 287 & 402 & 15\n",
    "               5 & 203 & 495 & 21\n",
    "               6 & 58 & 173 & 15\n",
    "               7 & 210 & 479 & 27\n",
    "               8 & 202 & 504 & 14\n",
    "               9 & 198 & 510 & 30\n",
    "               10 & 158 & 416 & 16\n",
    "               11 & 165 & 393 & 14\n",
    "               12 & 201 & 442 & 25\n",
    "               13 & 157 & 317 & 52\n",
    "               14 & 131 & 311 & 16\n",
    "               15 & 166 & 400 & 34\n",
    "               16 & 160 & 337 & 31\n",
    "               17 & 186 & 423 & 42\n",
    "               18 & 125 & 334 & 26\n",
    "               19 & 218 & 533 & 16\n",
    "               20 & 146 & 344 & 22\"\"\"\n",
    "data= []\n",
    "for line in datastring.split('\\n'):\n",
    "    data.append([float(f) for f in line.split('&')])\n",
    "data= numpy.array(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the necessary arrays, ${\\bf Y}$, ${\\bf A}$, and ${\\bf C}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y= data[:,2]\n",
    "A= numpy.vstack((numpy.ones_like(Y),data[:,1])).T\n",
    "C= numpy.diag(data[:,3]**2.)\n",
    "X= np.matmul( np.linalg.inv( np.linalg.multi_dot([A.T,np.linalg.inv(C),A]) ), np.linalg.multi_dot([A.T,np.linalg.inv(C),Y]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "A0 = numpy.vstack((numpy.zeros_like(Y),data[:,1])).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0.        , 3447.19353805])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.multi_dot([A0.T,np.linalg.inv(C),A,X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0.        , 3447.19353805])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.multi_dot([X.T,A.T,np.linalg.inv(C),A0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the solution, ${\\bf W} = [{\\bf A}^T {\\bf C}^{-1}{\\bf A}]^{-1} {\\bf A}^T {\\bf C}^{-1}{\\bf  Y}$, and its uncertainty covariance ${\\bf V} = ({\\bf A}^T {\\bf C}^{-1}{\\bf A})^{-1}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "V= numpy.linalg.inv(numpy.dot(A.T,numpy.dot(numpy.linalg.inv(C),A)))\n",
    "W= numpy.dot(V,numpy.dot(A.T,numpy.dot(numpy.linalg.inv(C),Y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These agree with *Hogg et al. (2010; Fig. 2)*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[213.27349198   1.07674752]\n",
      "[14.39403311  0.07740678]\n"
     ]
    }
   ],
   "source": [
    "print(W)\n",
    "print(numpy.sqrt(numpy.diag(V)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Twice chi-squared is equal to $\\chi^2 = ({\\bf Y} - {\\bf A X})^T {\\bf C}^{-1}\\,({\\bf Y} - {\\bf A X})$ which when evaluating it for the solution ${\\bf X} = {\\bf W}$ is equal to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "289.9637227819993\n"
     ]
    }
   ],
   "source": [
    "YminusAW= Y-numpy.dot(A,W)\n",
    "twochi2= numpy.dot(YminusAW.T,numpy.dot(numpy.linalg.inv(C),YminusAW))\n",
    "print(twochi2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $\\chi^2$ is also equal to \n",
    "\n",
    "\\begin{equation}\n",
    "\\chi^2 = ({\\bf X} - [{\\bf A}^T {\\bf C}^{-1}{\\bf A}]^{-1} {\\bf A}^T {\\bf C}^{-1}{\\bf  Y} )^T [{\\bf A}^T {\\bf C}^{-1} {\\bf A}] ({\\bf X} - [{\\bf A}^T {\\bf C}^{-1}{\\bf A}]^{-1} {\\bf A}^T {\\bf C}^{-1}{\\bf  Y} )-{\\bf Y}^T {\\bf C}^{-1} {\\bf A} [{\\bf A}^T {\\bf C}^{-1}{\\bf A}]^{-1} {\\bf A}^T {\\bf C}^{-1}{\\bf Y} + {\\bf Y}^T {\\bf C}^{-1}{\\bf  Y}\\,.\n",
    "\\end{equation}\n",
    "\n",
    "That is, when writing it as $({\\bf X} - {\\bf W})^T {\\bf V}^{-1}\\,({\\bf X} - {\\bf W}) + {\\bf U}$ we have\n",
    "\\begin{equation}\n",
    "    {\\bf W} = [{\\bf A}^T {\\bf C}^{-1}{\\bf A}]^{-1} {\\bf A}^T {\\bf C}^{-1}{\\bf  Y}\\,,\n",
    "\\end{equation}\n",
    "\n",
    "as well as\n",
    "\n",
    "\\begin{equation}\n",
    "    {\\bf V} = ({\\bf A}^T {\\bf C}^{-1}{\\bf A})^{-1}\\,,\n",
    "\\end{equation}\n",
    "\n",
    "and also\n",
    "\n",
    "\\begin{equation}\n",
    "    {\\bf U} = -{\\bf Y}^T {\\bf C}^{-1} {\\bf A} [{\\bf A}^T {\\bf C}^{-1}{\\bf A}]^{-1} {\\bf A}^T {\\bf C}^{-1}{\\bf Y} + {\\bf Y}^T {\\bf C}^{-1}{\\bf  Y}\\,,\n",
    "\\end{equation}\n",
    "\n",
    "which we can also write as\n",
    "\n",
    "\\begin{equation}\n",
    "    {\\bf U} = {\\bf Y}^T {\\bf C}^{-1} ({\\bf Y}-{\\bf A} {\\bf W})\\,,\n",
    "\\end{equation}\n",
    "\n",
    "You can back out ${\\bf U}$ as being, in general:\n",
    "\n",
    "\\begin{equation}\n",
    "    {\\bf U} = {\\bf Y}^T {\\bf C}^{-1} {\\bf Y} - {\\bf W}^{T} {\\bf V}^{-1} {\\bf W}\n",
    "\\end{equation}\n",
    "\n",
    "By keeping ${\\bf W}$ un-expanded when the original $\\chi^{2}$ is equated with the new version\n",
    "\n",
    "For the solution, the first term is zero and we are left with ${\\bf U}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "289.96372278199215\n"
     ]
    }
   ],
   "source": [
    "new_twochi2= numpy.dot(Y.T,numpy.dot(numpy.linalg.inv(C),YminusAW))\n",
    "print(new_twochi2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This agrees with the directly calculated value above. Notice that ${\\bf U}$ is *very* similar to the original $\\chi^2$ that we calculated. In fact, we can show that we can just as easily write ${\\bf U}$ as\n",
    "\n",
    "\\begin{equation}\n",
    "        {\\bf U} = ({\\bf Y}-{\\bf A} {\\bf W})^T {\\bf C}^{-1} ({\\bf Y}-{\\bf A} {\\bf W})\\,,\n",
    "\\end{equation}\n",
    "\n",
    "This equation becomes two terms when the bracket is multiplied out. The first of those terms is ${\\bf U}$ above. The second is equal to 0:\n",
    "\n",
    "\\begin{equation}\n",
    "        ({\\bf A} {\\bf W})^T {\\bf C}^{-1} ({\\bf Y}-{\\bf A} {\\bf W}) = 0\\,.\n",
    "\\end{equation}\n",
    "\n",
    "This can be shown by substituting ${\\bf W}$ and ${\\bf W}^T$, yielding the identity:\n",
    "\n",
    "\\begin{equation}\n",
    "    {\\bf Y}^T {\\bf C}^{-1} {\\bf A} [ {\\bf A}^T {\\bf C}^{-1} A ]^{-1} {\\bf A}^T {\\bf C}^{-1} {\\bf Y} = {\\bf Y}^T {\\bf C}^{-1} {\\bf A} [ {\\bf A}^T {\\bf C}^{-1} A ]^{-1} {\\bf A}^T {\\bf C}^{-1} A [ {\\bf A}^T {\\bf C}^{-1} A ]^{-1} {\\bf A}^T {\\bf C}^{-1} {\\bf Y}\n",
    "\\end{equation}\n",
    "\n",
    "Where two of the central brackets on the right hand side are inverses of one another and cancel. That this is the case follows directly from the derivation of the solution ${\\bf W}$ as the maximum likelihood solution. We can also test that the above equation is equal to 0 numerically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-7.208456054286216e-12\n"
     ]
    }
   ],
   "source": [
    "print(numpy.dot(numpy.dot(A,W).T,numpy.dot(numpy.linalg.inv(C),YminusAW)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we have that\n",
    "\n",
    "\\begin{equation}\n",
    "    ({\\bf Y} - {\\bf A X})^T {\\bf C}^{-1}\\,({\\bf Y} - {\\bf A X}) = ({\\bf X} - {\\bf W})^T {\\bf V}^{-1}\\,({\\bf X} - {\\bf W}) + ({\\bf Y} - {\\bf A W})^T {\\bf C}^{-1}\\,({\\bf Y} - {\\bf A W})\\,.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "289.9637227819993\n",
      "0.0\n",
      "289.9637227819993\n"
     ]
    }
   ],
   "source": [
    "print( np.linalg.multi_dot( [(Y-np.matmul(A,X)).T,np.linalg.inv(C),(Y-np.matmul(A,X))] ) )\n",
    "print( np.linalg.multi_dot( [(X-W).T,np.linalg.inv(V),(X-W)] ) )\n",
    "print( np.linalg.multi_dot( [(Y-np.matmul(A,W)).T,np.linalg.inv(C),(Y-np.matmul(A,W))] ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doing the same thing with a prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If now we perform the same marginalization, but with a Gaussian prior of the form:\n",
    "\n",
    "\\begin{equation}\n",
    "    ( {\\bf X} - {\\bf X}_{0} )^{T} {\\bf \\Sigma}^{-1} ( {\\bf X} - {\\bf X}_{0} )\n",
    "\\end{equation}\n",
    "\n",
    "For this prior, ${\\bf X}_{0}$ is a column vector of the same shape as ${\\bf X}$, which sets the mean value of the prior (i.e. $m$ or $b$). The variance, ${\\bf \\Sigma}$, sets the width of the Gaussian prior. For a flat uniform prior on a parameter set the corresponding element of ${\\bf X}_{0}$ equal to 0 (or anything technically), and set the element of the $N \\times N$ inverse variance matrix ${\\bf \\Sigma}^{-1}$ equal to 0 (i.e. the standard deviation for that parameter is infinity).\n",
    "\n",
    "The new ${\\bf V}$ takes the form:\n",
    "\n",
    "\\begin{equation}\n",
    "    {\\bf V}^{-1} = {\\bf A}^{T} {\\bf C}^{-1} {\\bf A} + {\\bf \\Sigma}^{-1}\n",
    "\\end{equation}\n",
    "\n",
    "i.e. the inverse variance matrices add linear.\n",
    "\n",
    "The new ${\\bf W}$ takes the form:\n",
    "\n",
    "\\begin{equation}\n",
    "    {\\bf W} = ( {\\bf A}^{T} {\\bf C}^{-1} {\\bf A} + {\\bf \\Sigma}^{-1} )^{-1}( {\\bf A}^{T} {\\bf C}^{-1} {\\bf Y} + {\\bf \\Sigma}^{-1} {\\bf X}_{0} )\n",
    "\\end{equation}\n",
    "\n",
    "Which is basically equivalent to the multivariate normal distribution product rules, for example as found in http://compbio.fmph.uniba.sk/vyuka/ml/old/2008/handouts/matrix-cookbook.pdf Section 8.1.8\n",
    "\n",
    "Note that this is also the linear algebra solution to the linear best-fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/cb11d976d3cdb50ab3afa609f88349d0"
  },
  "gist": {
   "data": {
    "description": "Fix chi2",
    "public": false
   },
   "id": "cb11d976d3cdb50ab3afa609f88349d0"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
